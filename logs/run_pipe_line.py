# fully generated by claude, seems interesting but it will not allow me to learn it myself so I just put it here. Maybe later on I can take some useful tricks/parts from here
"""
=============================================================================
IMAGE-TO-MEL SPECTROGRAM DiT PIPELINE
=============================================================================
Goal: Train a Diffusion Transformer to generate Mel Spectrograms conditioned 
      on raw patchified image pixels.

Architecture Overview:
----------------------
    [Image 512x512]          [Random Noise / Noisy Mel]
          |                            |
          v                            v
    [Patchify 16x16]            [Patchify 16x16]
          |                            |
          v                            v
    [1024 tokens x 768]         [256 tokens x 1280]
          |                            |
          +--------> DiT <-------------+
                      |
                      v
              [Predicted Noise]
                      |
                      v
              [Denoised Mel Spectrogram]
                      |
                      v (Vocoder - not in this script)
              [Audio Waveform]

=============================================================================
"""

# %% [markdown]
# # CELL 1: IMPORTS AND SETUP

# %%
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import torchaudio.transforms as T
from torchvision import transforms
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, Tuple, List
import math

# Check GPU availability
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Number of GPUs: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"\nUsing device: {DEVICE}")


# %% [markdown]
# # CELL 2: CONFIGURATION

# %%
@dataclass
class Config:
    """All hyperparameters in one place"""
    
    # Image settings
    image_size: int = 512
    image_patch_size: int = 16  # 512/16 = 32x32 = 1024 patches
    image_channels: int = 3
    
    # Mel spectrogram settings
    mel_n_mels: int = 80           # Standard mel bins
    mel_n_fft: int = 1024
    mel_hop_length: int = 256
    mel_target_frames: int = 64   # Fixed length (â‰ˆ3 seconds at 22kHz)
    mel_sample_rate: int = 22050
    mel_patch_size: int = 4        # Patch mel as 4x4 -> 20x64 = 1280 patches
    
    # DiT architecture
    hidden_size: int = 512         # Not too large for 4090s
    num_heads: int = 8
    num_layers: int = 8            # Moderate depth
    mlp_ratio: float = 4.0
    dropout: float = 0.1
    
    # Conditioning
    image_token_dim: int = 768     # 16*16*3 = 768 (raw patch pixels)
    cond_proj_dim: int = 512       # Project image tokens to this dim
    
    # Diffusion
    num_timesteps: int = 1000
    beta_start: float = 0.0001
    beta_end: float = 0.02
    
    # Training
    batch_size: int = 4
    learning_rate: float = 1e-4
    num_epochs: int = 500
    
    @property
    def num_image_patches(self) -> int:
        """Number of patches from image"""
        return (self.image_size // self.image_patch_size) ** 2  # 1024
    
    @property
    def num_mel_patches(self) -> int:
        """Number of patches from mel spectrogram"""
        h_patches = self.mel_n_mels // self.mel_patch_size      # 80/4 = 20
        w_patches = self.mel_target_frames // self.mel_patch_size  # 256/4 = 64
        return h_patches * w_patches  # 1280
    
    @property
    def mel_patch_dim(self) -> int:
        """Dimension of each mel patch"""
        return self.mel_patch_size * self.mel_patch_size  # 4*4 = 16

config = Config()
print("Configuration:")
print(f"  Image: {config.image_size}x{config.image_size} -> {config.num_image_patches} patches of {config.image_patch_size}x{config.image_patch_size}x{config.image_channels} = {config.image_token_dim}D")
print(f"  Mel: {config.mel_n_mels}x{config.mel_target_frames} -> {config.num_mel_patches} patches of {config.mel_patch_size}x{config.mel_patch_size} = {config.mel_patch_dim}D")
print(f"  DiT: {config.num_layers} layers, {config.hidden_size}D, {config.num_heads} heads")


# %% [markdown]
# # CELL 3: DATA PROCESSING UTILITIES

# %%
class ImageProcessor:
    """Process images into patchified sequences for DiT conditioning"""
    
    def __init__(self, config: Config):
        self.config = config
        self.transform = transforms.Compose([
            transforms.Resize((config.image_size, config.image_size)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # -> [-1, 1]
        ])
    
    def load_image(self, path: str) -> torch.Tensor:
        """Load and transform image to tensor"""
        img = Image.open(path).convert("RGB")
        return self.transform(img)  # (3, 512, 512)
    
    def patchify(self, x: torch.Tensor) -> torch.Tensor:
        """
        Convert image to sequence of patch tokens.
        
        Input: (B, C, H, W) or (C, H, W)
        Output: (B, num_patches, patch_dim) where patch_dim = C * P * P
        
        Visual explanation:
        
        Image (3, 512, 512):
        +--+--+--+--+...+--+
        |  |  |  |  |   |  |  <- 32 patches wide
        +--+--+--+--+...+--+
        |  |  |  |  |   |  |
        +--+--+--+--+...+--+
        ...                    <- 32 patches tall
        +--+--+--+--+...+--+
        
        Each patch is 16x16x3 = 768 values
        Total: 32x32 = 1024 patches
        Output: (B, 1024, 768)
        """
        if x.dim() == 3:
            x = x.unsqueeze(0)  # Add batch dim
            
        B, C, H, W = x.shape
        P = self.config.image_patch_size
        
        # Reshape: (B, C, H/P, P, W/P, P)
        x = x.reshape(B, C, H // P, P, W // P, P)
        # Permute: (B, H/P, W/P, C, P, P)
        x = x.permute(0, 2, 4, 1, 3, 5)
        # Flatten patches: (B, num_patches, C*P*P)
        x = x.reshape(B, -1, C * P * P)
        
        return x
    
    def process(self, path: str) -> torch.Tensor:
        """Full pipeline: load -> transform -> patchify"""
        img = self.load_image(path)
        patches = self.patchify(img)
        return patches.squeeze(0)  # (1024, 768)


class MelProcessor:
    """Process audio to mel spectrograms and patchify for DiT"""
    
    def __init__(self, config: Config):
        self.config = config
        self.mel_transform = T.MelSpectrogram(
            sample_rate=config.mel_sample_rate,
            n_fft=config.mel_n_fft,
            hop_length=config.mel_hop_length,
            n_mels=config.mel_n_mels,
            normalized=True
        )
    
    def load_audio(self, path: str) -> Tuple[torch.Tensor, int]:
        """Load audio file and resample if needed"""
        waveform, sr = torchaudio.load(path)
        
        # Convert to mono if stereo
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        # Resample if needed
        if sr != self.config.mel_sample_rate:
            resampler = T.Resample(sr, self.config.mel_sample_rate)
            waveform = resampler(waveform)
        
        return waveform, self.config.mel_sample_rate
    
    def to_mel(self, waveform: torch.Tensor) -> torch.Tensor:
        """
        Convert waveform to log-mel spectrogram.
        
        Input: (1, num_samples)
        Output: (1, n_mels, target_frames) = (1, 80, 256)
        """
        # Compute mel spectrogram
        mel = self.mel_transform(waveform)  # (1, 80, time)
        
        # Log scale
        mel = torch.log(mel + 1e-9)
        
        # Normalize to [-1, 1]
        mel = (mel - mel.mean()) / (mel.std() + 1e-9)
        mel = torch.clamp(mel, -3, 3) / 3  # Clip outliers, scale to [-1, 1]
        
        # Pad or truncate to fixed length
        target_len = self.config.mel_target_frames
        if mel.shape[-1] < target_len:
            mel = F.pad(mel, (0, target_len - mel.shape[-1]))
        else:
            mel = mel[..., :target_len]
        
        return mel  # (1, 80, 256)
    
    def patchify(self, mel: torch.Tensor) -> torch.Tensor:
        """
        Convert mel spectrogram to sequence of patches.
        
        Input: (B, 1, H, W) = (B, 1, 80, 256)
        Output: (B, num_patches, patch_dim) = (B, 1280, 16)
        
        Visual explanation:
        
        Mel (1, 80, 256):
        +--+--+--+--+...+--+
        |  |  |  |  |   |  |  <- 64 patches wide (256/4)
        +--+--+--+--+...+--+
        |  |  |  |  |   |  |
        ...                    <- 20 patches tall (80/4)
        +--+--+--+--+...+--+
        
        Each patch is 4x4 = 16 values
        Total: 20x64 = 1280 patches
        """
        if mel.dim() == 3:
            mel = mel.unsqueeze(0)  # (1, 1, 80, 256)
        
        B, C, H, W = mel.shape
        P = self.config.mel_patch_size
        
        # Reshape: (B, C, H/P, P, W/P, P)
        mel = mel.reshape(B, C, H // P, P, W // P, P)
        # Permute: (B, H/P, W/P, C, P, P)
        mel = mel.permute(0, 2, 4, 1, 3, 5)
        # Flatten: (B, num_patches, C*P*P)
        mel = mel.reshape(B, -1, C * P * P)
        
        return mel  # (B, 1280, 16)
    
    def unpatchify(self, patches: torch.Tensor) -> torch.Tensor:
        """
        Reverse patchification.
        
        Input: (B, 1280, 16)
        Output: (B, 1, 80, 256)
        """
        B = patches.shape[0]
        P = self.config.mel_patch_size
        H = self.config.mel_n_mels
        W = self.config.mel_target_frames
        H_P = H // P  # 20
        W_P = W // P  # 64
        
        # Reshape: (B, H/P, W/P, 1, P, P)
        x = patches.reshape(B, H_P, W_P, 1, P, P)
        # Permute: (B, 1, H/P, P, W/P, P)
        x = x.permute(0, 3, 1, 4, 2, 5)
        # Reshape: (B, 1, H, W)
        x = x.reshape(B, 1, H, W)
        
        return x
    
    def process(self, path: str) -> torch.Tensor:
        """Full pipeline: load -> mel -> patchify"""
        waveform, _ = self.load_audio(path)
        mel = self.to_mel(waveform)
        patches = self.patchify(mel)
        return patches.squeeze(0)  # (1280, 16)


# Test the processors
print("\n--- Testing Processors ---")
img_proc = ImageProcessor(config)
mel_proc = MelProcessor(config)

# Create dummy data for testing
dummy_image = torch.randn(3, 512, 512)
dummy_audio = torch.randn(1, 22050 * 3)  # 3 seconds

img_patches = img_proc.patchify(dummy_image)
print(f"Image patches shape: {img_patches.shape}")  # (1, 1024, 768)

mel = mel_proc.to_mel(dummy_audio)
print(f"Mel shape: {mel.shape}")  # (1, 80, 256)

mel_patches = mel_proc.patchify(mel)
print(f"Mel patches shape: {mel_patches.shape}")  # (1, 1280, 16)

mel_reconstructed = mel_proc.unpatchify(mel_patches)
print(f"Mel reconstructed shape: {mel_reconstructed.shape}")  # (1, 1, 80, 256)


# %% [markdown]
# # CELL 4: DIFFUSION UTILITIES

# %%
class DiffusionSchedule:
    """
    Diffusion noise schedule and utilities.
    
    The diffusion process gradually adds noise to clean data:
    
    x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon
    
    where:
    - x_0 is the clean mel spectrogram
    - x_t is the noisy version at timestep t
    - epsilon is random Gaussian noise
    - alpha_bar_t controls signal-to-noise ratio
    """
    
    def __init__(self, config: Config):
        self.num_timesteps = config.num_timesteps
        
        # Linear beta schedule
        betas = torch.linspace(config.beta_start, config.beta_end, config.num_timesteps)
        
        # Precompute useful quantities
        alphas = 1.0 - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)
        
        self.register_buffer('betas', betas)
        self.register_buffer('alphas', alphas)
        self.register_buffer('alphas_cumprod', alphas_cumprod)
        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))
        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))
        
    def register_buffer(self, name: str, tensor: torch.Tensor):
        """Store tensor as attribute"""
        setattr(self, name, tensor)
    
    def to(self, device):
        """Move all tensors to device"""
        for attr in ['betas', 'alphas', 'alphas_cumprod', 
                     'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod']:
            setattr(self, attr, getattr(self, attr).to(device))
        return self
    
    def add_noise(self, x_0: torch.Tensor, t: torch.Tensor, noise: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Add noise to clean data according to schedule.
        
        Args:
            x_0: Clean data (B, seq_len, dim)
            t: Timesteps (B,)
            noise: Optional pre-generated noise
            
        Returns:
            x_t: Noisy data
            noise: The noise that was added
        """
        if noise is None:
            noise = torch.randn_like(x_0)
        
        # Get coefficients for these timesteps
        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1)
        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1)
        
        # Add noise: x_t = sqrt(alpha_bar) * x_0 + sqrt(1 - alpha_bar) * noise
        x_t = sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise
        
        return x_t, noise
    
    def sample_timesteps(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """Sample random timesteps for training"""
        return torch.randint(0, self.num_timesteps, (batch_size,), device=device)


# Visualize the schedule
schedule = DiffusionSchedule(config)

plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.plot(schedule.betas.numpy())
plt.title("Beta (noise rate)")
plt.xlabel("Timestep")

plt.subplot(1, 3, 2)
plt.plot(schedule.alphas_cumprod.numpy())
plt.title("Alpha_bar (signal retention)")
plt.xlabel("Timestep")

plt.subplot(1, 3, 3)
# Show noising process
x_clean = torch.zeros(1, 10, 16)  # dummy clean signal
timesteps_to_show = [0, 250, 500, 750, 999]
for t in timesteps_to_show:
    t_tensor = torch.tensor([t])
    x_noisy, _ = schedule.add_noise(x_clean, t_tensor)
    plt.plot(x_noisy[0, 0, :].numpy(), label=f't={t}', alpha=0.7)
plt.title("Signal at different timesteps")
plt.legend()
plt.tight_layout()
plt.show()


# %% [markdown]
# # CELL 5: DiT MODEL ARCHITECTURE

# %%
class SinusoidalPositionEmbeddings(nn.Module):
    """Timestep embeddings using sinusoidal encoding"""
    
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
    
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        device = t.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = t[:, None] * embeddings[None, :]
        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)
        return embeddings


class AdaLayerNorm(nn.Module):
    """
    Adaptive Layer Norm that modulates based on timestep.
    
    This is a key component of DiT - it injects timestep information
    by scaling and shifting the normalized activations.
    """
    
    def __init__(self, hidden_size: int):
        super().__init__()
        self.norm = nn.LayerNorm(hidden_size, elementwise_affine=False)
        # Project timestep to scale and shift parameters
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(hidden_size, 2 * hidden_size)
        )
    
    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        # Get modulation parameters from timestep embedding
        modulation = self.adaLN_modulation(t_emb)  # (B, 2*hidden)
        scale, shift = modulation.chunk(2, dim=-1)  # Each (B, hidden)
        
        # Normalize and modulate
        x = self.norm(x)
        x = x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)
        return x


class CrossAttention(nn.Module):
    """
    Cross-attention layer for conditioning on image patches.
    
    This is where the magic happens for image->mel:
    - Query comes from mel patches (what we're generating)
    - Key/Value come from image patches (what we're conditioning on)
    
    The attention asks: "For this mel patch, which image patches are relevant?"
    """
    
    def __init__(self, hidden_size: int, cond_dim: int, num_heads: int, dropout: float = 0.0):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scale = self.head_dim ** -0.5
        
        # Query from mel features
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        # Key/Value from image features
        self.k_proj = nn.Linear(cond_dim, hidden_size)
        self.v_proj = nn.Linear(cond_dim, hidden_size)
        
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Mel features (B, mel_seq, hidden)
            cond: Image features (B, img_seq, cond_dim)
        """
        B, N, _ = x.shape
        M = cond.shape[1]  # Number of image patches
        
        # Project
        q = self.q_proj(x).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(cond).reshape(B, M, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(cond).reshape(B, M, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Attention
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)
        
        # Combine
        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)
        x = self.out_proj(x)
        
        return x


class SelfAttention(nn.Module):
    """Standard self-attention for mel sequence"""
    
    def __init__(self, hidden_size: int, num_heads: int, dropout: float = 0.0):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scale = self.head_dim ** -0.5
        
        self.qkv_proj = nn.Linear(hidden_size, 3 * hidden_size)
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, _ = x.shape
        
        qkv = self.qkv_proj(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)
        
        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)
        x = self.out_proj(x)
        
        return x


class MLP(nn.Module):
    """Feed-forward network"""
    
    def __init__(self, hidden_size: int, mlp_ratio: float = 4.0, dropout: float = 0.0):
        super().__init__()
        mlp_hidden = int(hidden_size * mlp_ratio)
        self.fc1 = nn.Linear(hidden_size, mlp_hidden)
        self.fc2 = nn.Linear(mlp_hidden, hidden_size)
        self.act = nn.GELU()
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x


class DiTBlock(nn.Module):
    """
    Single DiT block with:
    1. AdaLN -> Self-Attention (mel attends to itself)
    2. AdaLN -> Cross-Attention (mel attends to image)
    3. AdaLN -> MLP
    """
    
    def __init__(self, hidden_size: int, cond_dim: int, num_heads: int, 
                 mlp_ratio: float = 4.0, dropout: float = 0.0):
        super().__init__()
        
        # Self-attention block
        self.norm1 = AdaLayerNorm(hidden_size)
        self.self_attn = SelfAttention(hidden_size, num_heads, dropout)
        
        # Cross-attention block
        self.norm2 = AdaLayerNorm(hidden_size)
        self.cross_attn = CrossAttention(hidden_size, cond_dim, num_heads, dropout)
        
        # MLP block
        self.norm3 = AdaLayerNorm(hidden_size)
        self.mlp = MLP(hidden_size, mlp_ratio, dropout)
    
    def forward(self, x: torch.Tensor, cond: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        # Self-attention
        x = x + self.self_attn(self.norm1(x, t_emb))
        # Cross-attention to image
        x = x + self.cross_attn(self.norm2(x, t_emb), cond)
        # MLP
        x = x + self.mlp(self.norm3(x, t_emb))
        return x


class ImageToMelDiT(nn.Module):
    """
    Diffusion Transformer for Image-to-Mel generation.
    
    The model learns to denoise mel spectrograms conditioned on images.
    
    Forward pass:
    1. Patchify noisy mel -> sequence of tokens
    2. Add positional embeddings
    3. Process image patches (raw pixels) through projection
    4. Pass through DiT blocks with cross-attention to image
    5. Project output to predict noise
    6. Unpatchify back to mel shape
    """
    
    def __init__(self, config: Config):
        super().__init__()
        self.config = config
        
        # Input projections
        # Mel patches: (B, 1280, 16) -> (B, 1280, hidden)
        self.mel_proj = nn.Linear(config.mel_patch_dim, config.hidden_size)
        
        # Image patches: (B, 1024, 768) -> (B, 1024, cond_proj_dim)
        # This is the ONLY processing we do on raw image patches
        self.img_proj = nn.Linear(config.image_token_dim, config.cond_proj_dim)
        
        # Positional embeddings
        self.mel_pos_embed = nn.Parameter(
            torch.randn(1, config.num_mel_patches, config.hidden_size) * 0.02
        )
        self.img_pos_embed = nn.Parameter(
            torch.randn(1, config.num_image_patches, config.cond_proj_dim) * 0.02
        )
        
        # Timestep embedding
        self.time_embed = nn.Sequential(
            SinusoidalPositionEmbeddings(config.hidden_size),
            nn.Linear(config.hidden_size, config.hidden_size),
            nn.SiLU(),
            nn.Linear(config.hidden_size, config.hidden_size)
        )
        
        # DiT blocks
        self.blocks = nn.ModuleList([
            DiTBlock(
                config.hidden_size, 
                config.cond_proj_dim,
                config.num_heads,
                config.mlp_ratio,
                config.dropout
            )
            for _ in range(config.num_layers)
        ])
        
        # Output
        self.final_norm = nn.LayerNorm(config.hidden_size)
        self.out_proj = nn.Linear(config.hidden_size, config.mel_patch_dim)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        # Initialize projections
        for module in [self.mel_proj, self.img_proj, self.out_proj]:
            nn.init.xavier_uniform_(module.weight)
            nn.init.zeros_(module.bias)
        
        # Zero-init output projection for stable training start
        nn.init.zeros_(self.out_proj.weight)
        nn.init.zeros_(self.out_proj.bias)
    
    def forward(self, 
                noisy_mel: torch.Tensor,    # (B, 1280, 16)
                image_patches: torch.Tensor, # (B, 1024, 768)
                timesteps: torch.Tensor      # (B,)
               ) -> torch.Tensor:
        """
        Predict noise in noisy mel spectrogram.
        
        Returns: Predicted noise (B, 1280, 16)
        """
        # Get timestep embedding
        t_emb = self.time_embed(timesteps)  # (B, hidden)
        
        # Project inputs
        mel = self.mel_proj(noisy_mel) + self.mel_pos_embed  # (B, 1280, hidden)
        img = self.img_proj(image_patches) + self.img_pos_embed  # (B, 1024, cond_proj_dim)
        
        # Process through DiT blocks
        for block in self.blocks:
            mel = block(mel, img, t_emb)
        
        # Output
        mel = self.final_norm(mel)
        noise_pred = self.out_proj(mel)  # (B, 1280, 16)
        
        return noise_pred
    
    def count_parameters(self) -> int:
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


# Create and test model
model = ImageToMelDiT(config).to(DEVICE)
print(f"\nModel created with {model.count_parameters():,} parameters")

# Test forward pass
with torch.no_grad():
    dummy_mel = torch.randn(2, config.num_mel_patches, config.mel_patch_dim).to(DEVICE)
    dummy_img = torch.randn(2, config.num_image_patches, config.image_token_dim).to(DEVICE)
    dummy_t = torch.randint(0, 1000, (2,)).to(DEVICE)
    
    output = model(dummy_mel, dummy_img, dummy_t)
    print(f"Input mel shape: {dummy_mel.shape}")
    print(f"Input image shape: {dummy_img.shape}")
    print(f"Output shape: {output.shape}")


# %% [markdown]
# # CELL 6: DATASET CLASS

# %%
class ImageAudioDataset(torch.utils.data.Dataset):
    """
    Dataset for paired image-audio data.
    
    Expected directory structure:
    data_dir/
        image1.jpg
        image1.wav
        image2.png
        image2.wav
        ...
    
    Images and audio files should have the same base name.
    """
    
    def __init__(self, data_dir: str, config: Config):
        self.config = config
        self.data_dir = Path(data_dir)
        self.img_processor = ImageProcessor(config)
        self.mel_processor = MelProcessor(config)
        
        # Find all image-audio pairs
        self.pairs = []
        image_extensions = {'.jpg', '.jpeg', '.png', '.webp'}
        audio_extensions = {'.wav', '.mp3', '.flac'}
        
        for img_path in self.data_dir.iterdir():
            if img_path.suffix.lower() in image_extensions:
                stem = img_path.stem
                for audio_ext in audio_extensions:
                    audio_path = self.data_dir / f"{stem}{audio_ext}"
                    if audio_path.exists():
                        self.pairs.append((str(img_path), str(audio_path)))
                        break
        
        print(f"Found {len(self.pairs)} image-audio pairs in {data_dir}")
    
    def __len__(self) -> int:
        return len(self.pairs)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        img_path, audio_path = self.pairs[idx]
        
        # Process image to patches
        img_patches = self.img_processor.process(img_path)  # (1024, 768)
        
        # Process audio to mel patches
        mel_patches = self.mel_processor.process(audio_path)  # (1280, 16)
        
        return img_patches, mel_patches

class SinglePairDataset(torch.utils.data.Dataset):
    def __init__(self, img_path, audio_path, config, repeat=100):
        self.img_processor = ImageProcessor(config)
        self.mel_processor = MelProcessor(config)
        self.img_patches = self.img_processor.process(img_path)
        self.mel_patches = self.mel_processor.process(audio_path)
        self.repeat = repeat
    
    def __len__(self):
        return self.repeat
    
    def __getitem__(self, idx):
        return self.img_patches, self.mel_patches


class DummyDataset(torch.utils.data.Dataset):
    """Dummy dataset for testing when you don't have real data"""
    
    def __init__(self, num_samples: int, config: Config):
        self.num_samples = num_samples
        self.config = config
    
    def __len__(self) -> int:
        return self.num_samples
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # Random image patches
        img_patches = torch.randn(self.config.num_image_patches, self.config.image_token_dim)
        # Random mel patches
        mel_patches = torch.randn(self.config.num_mel_patches, self.config.mel_patch_dim)
        return img_patches, mel_patches


# %% [markdown]
# # CELL 7: TRAINING LOOP

# %%
def train_one_epoch(model: ImageToMelDiT, 
                    dataloader: torch.utils.data.DataLoader,
                    optimizer: torch.optim.Optimizer,
                    scheduler: DiffusionSchedule,
                    device: torch.device,
                    epoch: int) -> float:
    """
    Train for one epoch.
    
    Training objective:
    - Sample random timestep t
    - Add noise to clean mel: x_t = sqrt(alpha_bar) * x_0 + sqrt(1-alpha_bar) * noise
    - Predict the noise: noise_pred = model(x_t, image, t)
    - Loss = MSE(noise_pred, noise)
    """
    model.train()
    total_loss = 0.0
    
    for batch_idx, (img_patches, mel_patches) in enumerate(dataloader):
        img_patches = img_patches.to(device)  # (B, 1024, 768)
        mel_patches = mel_patches.to(device)  # (B, 1280, 16)
        
        B = img_patches.shape[0]
        
        # Sample random timesteps
        t = scheduler.sample_timesteps(B, device)
        
        # Add noise to mel
        noisy_mel, noise = scheduler.add_noise(mel_patches, t)
        
        # Predict noise
        noise_pred = model(noisy_mel, img_patches, t)
        
        # Compute loss
        loss = F.mse_loss(noise_pred, noise)
        
        # Backprop
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        total_loss += loss.item()
        
        if batch_idx % 10 == 0:
            print(f"  Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.6f}")
    
    return total_loss / len(dataloader)


@torch.no_grad()
def sample(model: ImageToMelDiT,
           image_patches: torch.Tensor,
           scheduler: DiffusionSchedule,
           num_steps: int = 50,
           device: torch.device = DEVICE) -> torch.Tensor:
    """
    Generate mel spectrogram from image using DDPM sampling.
    
    Sampling process:
    1. Start with pure noise
    2. Iteratively denoise by predicting and removing noise
    3. Return final clean mel
    """
    model.eval()
    B = image_patches.shape[0]
    
    # Start from pure noise
    mel = torch.randn(B, config.num_mel_patches, config.mel_patch_dim).to(device)
    
    # Use evenly spaced timesteps for sampling
    timesteps = torch.linspace(scheduler.num_timesteps - 1, 0, num_steps).long().to(device)
    
    for i, t in enumerate(timesteps):
        t_batch = t.repeat(B)
        
        # Predict noise
        noise_pred = model(mel, image_patches, t_batch)
        
        # Get alpha values
        alpha = scheduler.alphas[t]
        alpha_bar = scheduler.alphas_cumprod[t]
        beta = scheduler.betas[t]
        
        # DDPM update step
        if t > 0:
            noise = torch.randn_like(mel)
            alpha_bar_prev = scheduler.alphas_cumprod[t - 1]
        else:
            noise = torch.zeros_like(mel)
            alpha_bar_prev = torch.tensor(1.0)
        
        # Compute x_{t-1}
        # x_0 prediction
        x0_pred = (mel - torch.sqrt(1 - alpha_bar) * noise_pred) / torch.sqrt(alpha_bar)
        x0_pred = torch.clamp(x0_pred, -3, 3)  # Clip for stability
        
        # Compute mean
        mean = (torch.sqrt(alpha_bar_prev) * beta / (1 - alpha_bar)) * x0_pred + \
               (torch.sqrt(alpha) * (1 - alpha_bar_prev) / (1 - alpha_bar)) * mel
        
        # Add noise (except for t=0)
        var = beta * (1 - alpha_bar_prev) / (1 - alpha_bar)
        mel = mean + torch.sqrt(var) * noise
    
    return mel


def visualize_training_sample(model: ImageToMelDiT, 
                              img_patches: torch.Tensor,
                              mel_patches: torch.Tensor,
                              mel_processor: MelProcessor,
                              scheduler: DiffusionSchedule,
                              device: torch.device):
    """Visualize original vs generated mel"""
    
    # Generate from image
    img_patches = img_patches.unsqueeze(0).to(device)
    generated = sample(model, img_patches, scheduler, num_steps=50, device=device)
    
    # Unpatchify
    original_mel = mel_processor.unpatchify(mel_patches.unsqueeze(0))
    generated_mel = mel_processor.unpatchify(generated.cpu())
    
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.imshow(original_mel[0, 0].numpy(), aspect='auto', origin='lower', cmap='magma')
    plt.title("Original Mel Spectrogram")
    plt.colorbar()
    
    plt.subplot(1, 2, 2)
    plt.imshow(generated_mel[0, 0].numpy(), aspect='auto', origin='lower', cmap='magma'
    )
    plt.title("Generated Mel Spectrogram")
    plt.colorbar()
    
    plt.tight_layout()
    plt.show()


# %% [markdown]
# # CELL 8: MAIN TRAINING SCRIPT

# %%
def main():
    """Main training function"""
    
    # Configuration
    config = Config()
    device = DEVICE
    
    # Create model
    model = ImageToMelDiT(config).to(device)
    print(f"Model parameters: {model.count_parameters():,}")
    
    # Create schedule
    scheduler = DiffusionSchedule(config).to(device)
    
    # Create dataset
    # Option 1: Real data (uncomment and set path)
    # dataset = ImageAudioDataset("/path/to/your/data", config)
    
    # Option 2: Dummy data for testing
    dataset = SinglePairDataset(
        "/home/vladimir_albrekht/projects/img_to_spec/assets/animeha.jpg",
        "/home/vladimir_albrekht/projects/img_to_spec/assets/anime.wav",
        config,
        repeat=100
    )
    
    dataloader = torch.utils.data.DataLoader(
        dataset, 
        batch_size=config.batch_size, 
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config.learning_rate,
        weight_decay=0.01
    )
    
    # Training loop
    print("\n" + "="*50)
    print("Starting Training")
    print("="*50)
    
    losses = []
    for epoch in range(config.num_epochs):
        print(f"\nEpoch {epoch + 1}/{config.num_epochs}")
        
        avg_loss = train_one_epoch(
            model, dataloader, optimizer, scheduler, device, epoch
        )
        losses.append(avg_loss)
        
        print(f"Epoch {epoch + 1} Average Loss: {avg_loss:.6f}")
        
        # Visualize every 100 epochs
        if (epoch + 1) % 100 == 0:
            # Get a sample from dataset
            img_patches, mel_patches = dataset[0]
            visualize_training_sample(
                model, img_patches, mel_patches,
                MelProcessor(config), scheduler, device
            )
            
            # Save checkpoint
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, f'checkpoint_epoch_{epoch+1}.pt')
    
    # Plot loss curve
    plt.figure(figsize=(10, 4))
    plt.plot(losses)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training Loss")
    plt.show()
    
    return model, scheduler


# Run training
if __name__ == "__main__":
    trained_model, trained_scheduler = main()



# %% [markdown]
# # CELL 9: INFERENCE EXAMPLE

# %%
def inference_example(model: ImageToMelDiT, 
                      image_path: str,
                      config: Config,
                      scheduler: DiffusionSchedule,
                      device: torch.device):
    """
    Generate mel spectrogram from a single image.
    """
    # Process image
    img_processor = ImageProcessor(config)
    mel_processor = MelProcessor(config)
    
    # Load and patchify image
    img_patches = img_processor.process(image_path)  # (1024, 768)
    img_patches = img_patches.unsqueeze(0).to(device)  # (1, 1024, 768)
    
    # Generate mel
    print("Generating mel spectrogram...")
    generated_patches = sample(model, img_patches, scheduler, num_steps=100, device=device)
    
    # Unpatchify
    generated_mel = mel_processor.unpatchify(generated_patches.cpu())  # (1, 1, 80, 256)
    
    # Visualize
    plt.figure(figsize=(12, 4))
    plt.imshow(generated_mel[0, 0].numpy(), aspect='auto', origin='lower')
    plt.title(f"Generated Mel from: {image_path}")
    plt.xlabel("Time Frames")
    plt.ylabel("Mel Bins")
    plt.colorbar()
    plt.show()
    
    return generated_mel


# Example usage (uncomment with real paths):
generated = inference_example(
    trained_model, 
    "assets/animeha.jpg",
    config,
    trained_scheduler,
    DEVICE
)


# %% [markdown]
# # CELL 10: MEL TO AUDIO INFERENCE WITH VOCODER
# 
# This cell converts generated mel spectrograms to actual audio using HiFi-GAN vocoder.

# %%
import torch
import torchaudio
import matplotlib.pyplot as plt
import numpy as np
from IPython.display import Audio, display
import warnings
warnings.filterwarnings('ignore')

# ============================================
# PART 1: LOAD VOCODER (HiFi-GAN)
# ============================================

def load_hifigan_vocoder(device='cuda'):
    """
    Load HiFi-GAN vocoder from torch hub.
    HiFi-GAN converts mel spectrograms to waveforms.
    """
    print("Loading HiFi-GAN vocoder...")
    
    # Option 1: Use torchaudio's built-in vocoder (simpler)
    try:
        # Try to use the bundled vocoder
        bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH
        vocoder = bundle.get_vocoder().to(device)
        vocoder.eval()
        print("Loaded WaveRNN vocoder from torchaudio")
        return vocoder, "wavernn", bundle.sample_rate
    except:
        pass
    
    # Option 2: Load HiFi-GAN from torch hub
    try:
        vocoder = torch.hub.load(
            'bshall/hifigan:main', 
            'hifigan', 
            map_location=device
        )
        vocoder.eval()
        print("Loaded HiFi-GAN from torch hub")
        return vocoder, "hifigan", 22050
    except Exception as e:
        print(f"Could not load vocoder: {e}")
        print("Will use Griffin-Lim as fallback")
        return None, "griffin_lim", 22050


def griffin_lim_vocoder(mel_spec, n_fft=1024, hop_length=256, n_iter=32):
    """
    Griffin-Lim algorithm as fallback vocoder.
    Lower quality but no external dependencies.
    """
    # Convert log-mel back to linear
    mel_spec = mel_spec.squeeze()  # (80, 256)
    
    # Approximate inverse mel to get magnitude spectrogram
    # This is a rough approximation
    n_mels = mel_spec.shape[0]
    n_stft = n_fft // 2 + 1
    
    # Create approximate mel filterbank inverse
    mel_basis = torchaudio.functional.melscale_fbanks(
        n_freqs=n_stft,
        f_min=0,
        f_max=22050 // 2,
        n_mels=n_mels,
        sample_rate=22050
    )
    
    # Pseudo-inverse to go from mel back to linear
    mel_basis_pinv = torch.pinverse(mel_basis.T)
    
    # Un-normalize mel (reverse the normalization we did)
    mel_spec = mel_spec * 3  # Undo /3 clipping
    mel_spec = torch.exp(mel_spec)  # Undo log
    
    # Convert to linear spectrogram
    linear_spec = torch.matmul(mel_basis_pinv, mel_spec)
    linear_spec = torch.clamp(linear_spec, min=1e-9)
    
    # Griffin-Lim
    angles = torch.randn_like(linear_spec) * 2 * np.pi
    for _ in range(n_iter):
        complex_spec = linear_spec * torch.exp(1j * angles)
        waveform = torch.istft(
            complex_spec, 
            n_fft=n_fft, 
            hop_length=hop_length,
            return_complex=False
        )
        stft = torch.stft(
            waveform, 
            n_fft=n_fft, 
            hop_length=hop_length, 
            return_complex=True
        )
        angles = torch.angle(stft.squeeze())
    
    return waveform


# ============================================
# PART 2: PROPER MEL SPECTROGRAM PROCESSING
# ============================================

class ProperMelProcessor:
    """
    Mel processor that maintains proper scale for vocoder compatibility.
    """
    
    def __init__(self, config, sample_rate=22050):
        self.config = config
        self.sample_rate = sample_rate
        self.n_mels = config.mel_n_mels
        self.n_fft = config.mel_n_fft
        self.hop_length = config.mel_hop_length
        self.target_frames = config.mel_target_frames
        
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            n_mels=self.n_mels,
            f_min=0,
            f_max=sample_rate // 2,
            normalized=False  # Keep in natural dB scale
        )
    
    def audio_to_mel(self, audio_path):
        """Convert audio file to mel spectrogram"""
        waveform, sr = torchaudio.load(audio_path)
        
        # Mono
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        # Resample if needed
        if sr != self.sample_rate:
            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
            waveform = resampler(waveform)
        
        # Compute mel
        mel = self.mel_transform(waveform)  # (1, 80, T)
        
        # Log scale (dB)
        mel = torch.log(torch.clamp(mel, min=1e-5))
        
        # Normalize to roughly [-1, 1] range
        # These values are typical for speech/music
        mel = (mel + 5) / 5  # Shift and scale
        mel = torch.clamp(mel, -1, 1)
        
        # Pad/truncate to fixed length
        if mel.shape[-1] < self.target_frames:
            mel = torch.nn.functional.pad(mel, (0, self.target_frames - mel.shape[-1]))
        else:
            mel = mel[..., :self.target_frames]
        
        return mel, waveform
    
    def mel_to_audio_griffin_lim(self, mel, n_iter=60):
        """
        Convert mel spectrogram back to audio using Griffin-Lim.
        
        Args:
            mel: (1, 80, T) or (80, T) mel spectrogram in [-1, 1] range
        """
        if mel.dim() == 3:
            mel = mel.squeeze(0)  # (80, T)
        
        # Denormalize
        mel = mel * 5 - 5  # Reverse (mel + 5) / 5
        mel = torch.exp(mel)  # Reverse log
        if mel.dim() == 4:
        mel = mel.squeeze(0).squeeze(0)  # (1, 1, 80, 64) -> (80, 64)
    elif mel.dim() == 3:
        mel = mel.squeeze(0)  # (1, 80, 64) -> (80, 64)
        # Create mel filterbank
        mel_basis = torchaudio.functional.melscale_fbanks(
            n_freqs=self.n_fft // 2 + 1,
            f_min=0,
            f_max=self.sample_rate // 2,
            n_mels=self.n_mels,
            sample_rate=self.sample_rate
        )
        
        # Pseudo-inverse to approximate linear spectrogram
        mel_basis_pinv = torch.linalg.pinv(mel_basis.T)
        linear_spec = torch.matmul(mel_basis_pinv, mel)
        linear_spec = torch.clamp(linear_spec, min=1e-9)
        
        # Griffin-Lim iteration
        phase = torch.rand_like(linear_spec) * 2 * np.pi
        
        for i in range(n_iter):
            # Construct complex spectrogram
            complex_spec = linear_spec * torch.exp(1j * phase)
            
            # Inverse STFT
            waveform = torch.istft(
                complex_spec.unsqueeze(0),
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                length=None
            )
            
            # Forward STFT to get new phase
            new_stft = torch.stft(
                waveform,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                return_complex=True
            )
            phase = torch.angle(new_stft.squeeze(0))
            
            # Truncate phase to match linear_spec size
            phase = phase[:, :linear_spec.shape[1]]
        
        return waveform
    
    def patchify(self, mel):
        """Same as before - convert mel to patches"""
        if mel.dim() == 3:
            mel = mel.unsqueeze(0)
        
        B, C, H, W = mel.shape
        P = self.config.mel_patch_size
        
        mel = mel.reshape(B, C, H // P, P, W // P, P)
        mel = mel.permute(0, 2, 4, 1, 3, 5)
        mel = mel.reshape(B, -1, C * P * P)
        
        return mel
    
    def unpatchify(self, patches):
        """Same as before - convert patches back to mel"""
        B = patches.shape[0]
        P = self.config.mel_patch_size
        H = self.config.mel_n_mels
        W = self.config.mel_target_frames
        H_P = H // P
        W_P = W // P
        
        x = patches.reshape(B, H_P, W_P, 1, P, P)
        x = x.permute(0, 3, 1, 4, 2, 5)
        x = x.reshape(B, 1, H, W)
        
        return x


# ============================================
# PART 3: COMPLETE INFERENCE PIPELINE
# ============================================

def run_inference_with_audio(
    model,
    image_path: str,
    config,
    scheduler,
    device,
    num_sampling_steps: int = 100,
    use_griffin_lim: bool = True
):
    """
    Complete inference pipeline: Image â†’ Mel â†’ Audio
    
    Args:
        model: Trained DiT model
        image_path: Path to input image
        config: Model config
        scheduler: Diffusion scheduler
        device: torch device
        num_sampling_steps: Number of denoising steps
        use_griffin_lim: Use Griffin-Lim vocoder (True) or try HiFi-GAN (False)
    
    Returns:
        generated_mel: The generated mel spectrogram
        waveform: The generated audio waveform
    """
    model.eval()
    
    # Initialize processors
    img_processor = ImageProcessor(config)
    mel_processor = ProperMelProcessor(config)
    
    # 1. Process image
    print(f"Processing image: {image_path}")
    img_patches = img_processor.process(image_path)
    img_patches = img_patches.unsqueeze(0).to(device)
    print(f"  Image patches shape: {img_patches.shape}")
    
    # 2. Generate mel spectrogram
    print(f"Generating mel spectrogram ({num_sampling_steps} steps)...")
    generated_patches = sample(model, img_patches, scheduler, 
                               num_steps=num_sampling_steps, device=device)
    
    # 3. Unpatchify
    generated_mel = mel_processor.unpatchify(generated_patches.cpu())
    print(f"  Generated mel shape: {generated_mel.shape}")
    
    # 4. Convert to audio
    print("Converting mel to audio...")
    if use_griffin_lim:
        waveform = mel_processor.mel_to_audio_griffin_lim(generated_mel, n_iter=60)
        sample_rate = mel_processor.sample_rate
    else:
        # Try loading HiFi-GAN
        vocoder, voc_type, sample_rate = load_hifigan_vocoder(device)
        if vocoder is not None:
            with torch.no_grad():
                # Reshape mel for vocoder: (1, 80, T)
                mel_for_vocoder = generated_mel.squeeze(1).to(device)
                waveform = vocoder(mel_for_vocoder).cpu()
        else:
            waveform = mel_processor.mel_to_audio_griffin_lim(generated_mel, n_iter=60)
            sample_rate = mel_processor.sample_rate
    
    print(f"  Audio shape: {waveform.shape}, Sample rate: {sample_rate}")
    
    # 5. Visualize
    fig, axes = plt.subplots(2, 1, figsize=(14, 8))
    
    # Mel spectrogram
    mel_display = generated_mel[0, 0].numpy()
    im = axes[0].imshow(mel_display, aspect='auto', origin='lower', cmap='magma')
    axes[0].set_title('Generated Mel Spectrogram')
    axes[0].set_xlabel('Time Frames')
    axes[0].set_ylabel('Mel Bins')
    plt.colorbar(im, ax=axes[0])
    
    # Waveform
    if waveform.dim() > 1:
        waveform_plot = waveform.squeeze().numpy()
    else:
        waveform_plot = waveform.numpy()
    
    time_axis = np.arange(len(waveform_plot)) / sample_rate
    axes[1].plot(time_axis, waveform_plot, linewidth=0.5)
    axes[1].set_title('Generated Waveform')
    axes[1].set_xlabel('Time (seconds)')
    axes[1].set_ylabel('Amplitude')
    axes[1].set_xlim([0, time_axis[-1]])
    
    plt.tight_layout()
    plt.show()
    
    # 6. Play audio in notebook
    print("\nðŸ”Š Playing generated audio:")
    waveform_numpy = waveform.squeeze().numpy()
    display(Audio(waveform_numpy, rate=sample_rate))
    
    # 7. Save audio
    output_path = "generated_audio.wav"
    torchaudio.save(output_path, waveform.unsqueeze(0) if waveform.dim() == 1 else waveform, sample_rate)
    print(f"ðŸ’¾ Audio saved to: {output_path}")
    
    return generated_mel, waveform, sample_rate


# ============================================
# PART 4: TEST WITH REAL AUDIO FILE
# ============================================

def test_mel_pipeline_with_real_audio(audio_path: str, config):
    """
    Test the mel processing pipeline with a real audio file.
    This verifies that mel â†’ audio conversion works correctly.
    """
    print(f"Testing mel pipeline with: {audio_path}")
    
    mel_processor = ProperMelProcessor(config)
    
    # 1. Load audio and convert to mel
    mel, original_waveform = mel_processor.audio_to_mel(audio_path)
    print(f"  Original waveform shape: {original_waveform.shape}")
    print(f"  Mel spectrogram shape: {mel.shape}")
    print(f"  Mel value range: [{mel.min():.3f}, {mel.max():.3f}]")
    
    # 2. Convert mel back to audio
    reconstructed_waveform = mel_processor.mel_to_audio_griffin_lim(mel, n_iter=60)
    print(f"  Reconstructed waveform shape: {reconstructed_waveform.shape}")
    
    # 3. Visualize
    fig, axes = plt.subplots(3, 1, figsize=(14, 10))
    
    # Original waveform
    orig_np = original_waveform.squeeze().numpy()
    time_orig = np.arange(len(orig_np)) / mel_processor.sample_rate
    axes[0].plot(time_orig, orig_np, linewidth=0.5)
    axes[0].set_title('Original Waveform')
    axes[0].set_xlabel('Time (s)')
    
    # Mel spectrogram
    im = axes[1].imshow(mel.squeeze().numpy(), aspect='auto', origin='lower', cmap='magma')
    axes[1].set_title('Mel Spectrogram')
    axes[1].set_xlabel('Time Frames')
    axes[1].set_ylabel('Mel Bins')
    plt.colorbar(im, ax=axes[1])
    
    # Reconstructed waveform
    recon_np = reconstructed_waveform.squeeze().numpy()
    time_recon = np.arange(len(recon_np)) / mel_processor.sample_rate
    axes[2].plot(time_recon, recon_np, linewidth=0.5, color='orange')
    axes[2].set_title('Reconstructed Waveform (Griffin-Lim)')
    axes[2].set_xlabel('Time (s)')
    
    plt.tight_layout()
    plt.show()
    
    # Play both
    print("\nðŸ”Š Original audio:")
    display(Audio(orig_np, rate=mel_processor.sample_rate))
    
    print("\nðŸ”Š Reconstructed audio (from mel):")
    display(Audio(recon_np, rate=mel_processor.sample_rate))
    
    return mel, original_waveform, reconstructed_waveform


# ============================================
# USAGE EXAMPLES
# ============================================

# Example 1: Test mel pipeline with your audio file
# (Run this first to verify mel processing works)
"""
mel, orig_wav, recon_wav = test_mel_pipeline_with_real_audio(
    "/home/vladimir_albrekht/projects/img_to_spec/assets/anime.wav",
    config
)
"""

mel, orig_wav, recon_wav = test_mel_pipeline_with_real_audio(
    "/home/vladimir_albrekht/projects/img_to_spec/assets/anime.wav",
    config
)
# Example 2: Full inference with trained model
"""
generated_mel, waveform, sr = run_inference_with_audio(
    model=model,  # Your trained model
    image_path="assets/animeha.jpg",
    config=config,
    scheduler=scheduler,
    device=DEVICE,
    num_sampling_steps=100,
    use_griffin_lim=True
)
"""

scheduler = DiffusionSchedule(config).to(DEVICE)


generated_mel, waveform, sr = run_inference_with_audio(
    model=model,  # Your trained model
    image_path="assets/animeha.jpg",
    config=config,
    scheduler=scheduler,
    device=DEVICE,
    num_sampling_steps=100,
    use_griffin_lim=True
)
print("âœ… Inference cell loaded! Run the examples above.")


# %% [markdown]
# # UNDERSTANDING THE PIPELINE
# 
# ## Data Flow Diagram:
# 
# ```
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                        TRAINING PHASE                               â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚                                                                     â”‚
# â”‚  [Image 512x512]              [Audio .wav]                          â”‚
# â”‚       â”‚                            â”‚                                â”‚
# â”‚       â–¼                            â–¼                                â”‚
# â”‚  Resize & Normalize           Mel Spectrogram                       â”‚
# â”‚       â”‚                       (80x256)                              â”‚
# â”‚       â–¼                            â”‚                                â”‚
# â”‚  Patchify 16x16                    â–¼                                â”‚
# â”‚  (1024 patches)              Patchify 4x4                           â”‚
# â”‚       â”‚                      (1280 patches)                         â”‚
# â”‚       â–¼                            â”‚                                â”‚
# â”‚  Linear Proj                       â–¼                                â”‚
# â”‚  768 â†’ 512                   Linear Proj                            â”‚
# â”‚       â”‚                      16 â†’ 512                               â”‚
# â”‚       â”‚                            â”‚                                â”‚
# â”‚       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
# â”‚       â”‚              â”‚                                              â”‚
# â”‚       â”‚         Add Noise (t)                                       â”‚
# â”‚       â”‚              â”‚                                              â”‚
# â”‚       â”‚              â–¼                                              â”‚
# â”‚       â””â”€â”€â”€â”€â”€â”€â–º DiT Blocks â—„â”€â”€â”€â”€â”€â”€ Timestep Embedding                â”‚
# â”‚                    â”‚                                                â”‚
# â”‚                    â–¼                                                â”‚
# â”‚              Predict Noise                                          â”‚
# â”‚                    â”‚                                                â”‚
# â”‚                    â–¼                                                â”‚
# â”‚         MSE Loss(predicted, actual)                                 â”‚
# â”‚                                                                     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# 
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                       INFERENCE PHASE                               â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚                                                                     â”‚
# â”‚  [Image 512x512]              [Pure Noise]                          â”‚
# â”‚       â”‚                       (1280, 16)                            â”‚
# â”‚       â–¼                            â”‚                                â”‚
# â”‚  Patchify + Proj                   â”‚                                â”‚
# â”‚       â”‚                            â”‚                                â”‚
# â”‚       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
# â”‚       â”‚              â”‚                                              â”‚
# â”‚       â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚â—„â”€â”€â”€â”€ t=1000                                  â”‚
# â”‚       â”‚    â”‚         â–¼                                              â”‚
# â”‚       â””â”€â”€â”€â”€â”¼â”€â”€â”€â–º DiT Blocks                                         â”‚
# â”‚            â”‚         â”‚                                              â”‚
# â”‚            â”‚         â–¼                                              â”‚
# â”‚            â”‚   Predict Noise                                        â”‚
# â”‚            â”‚         â”‚                                              â”‚
# â”‚            â”‚         â–¼                                              â”‚
# â”‚            â”‚   Remove Noise                                         â”‚
# â”‚            â”‚         â”‚                                              â”‚
# â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (repeat for t=999, 998, ... 0)              â”‚
# â”‚                      â”‚                                              â”‚
# â”‚                      â–¼                                              â”‚
# â”‚              Clean Mel Patches                                      â”‚
# â”‚                      â”‚                                              â”‚
# â”‚                      â–¼                                              â”‚
# â”‚                 Unpatchify                                          â”‚
# â”‚                      â”‚                                              â”‚
# â”‚                      â–¼                                              â”‚
# â”‚             Mel Spectrogram                                         â”‚
# â”‚                 (80x256)                                            â”‚
# â”‚                      â”‚                                              â”‚
# â”‚                      â–¼                                              â”‚
# â”‚               [Vocoder] â”€â”€â”€â”€â–º Audio Waveform                        â”‚
# â”‚                                                                     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# ```
# 
# ## Key Points:
# 
# 1. **Image is CONDITIONING, not what's being denoised**
#    - The image patches go through cross-attention as Key/Value
#    - The mel patches are the Query - they "ask" the image what to generate
# 
# 2. **Raw pixel patches mean DiT must learn vision**
#    - Each image patch is 16Ã—16Ã—3 = 768 raw pixel values
#    - No pretrained features - model learns from scratch
#    - Requires more data than using CLIP/pretrained encoder
# 
# 3. **Patchification is just reshaping**
#    - Converts 2D spatial data to 1D sequence
#    - Enables transformer processing
#    - Unpatchify reverses the process
# 
# 4. **AdaLayerNorm injects timestep information**
#    - Tells the model "how noisy is this?"
#    - Different behavior at different noise levels
# %%
