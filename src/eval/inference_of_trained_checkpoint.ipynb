{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "INFERENCE\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir_albrekht/projects/img_to_spec/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/vladimir_albrekht/projects/img_to_spec/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image patches: torch.Size([1, 1024, 768])\n",
      "✓ Loaded model from epoch 50 with loss 0.0132\n",
      "Running inference with 100 steps...\n",
      "Timesteps: [999, 989, 979, 969, 959]...[49, 39, 29, 19, 9]\n",
      "✓ Generated mel shape: torch.Size([1, 1, 100, 280])\n",
      "Generated mel: torch.Size([1, 1, 100, 280])\n",
      "==================================================\n",
      "CONVERTING TO AUDIO\n",
      "==================================================\n",
      "Generated waveform: torch.Size([1, 71424])\n",
      "✓ Saved audio to /home/vladimir_albrekht/projects/img_to_spec/src/inference_output/generated_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir_albrekht/projects/img_to_spec/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "/home/vladimir_albrekht/projects/img_to_spec/.venv/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n"
     ]
    }
   ],
   "source": [
    "### INFERENCE (FIXED)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"INFERENCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import torch\n",
    "from debug_training_code import MelAdapter, NoiseScheduler, SimpleDiT, AudioProcessor\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from vocos import Vocos\n",
    "import torchaudio\n",
    "\n",
    "MODEL_PATH = \"/home/vladimir_albrekht/projects/img_to_spec/src/output/test_1/dit_checkpoint.pt\"\n",
    "INPUT_IMAGE = \"/home/vladimir_albrekht/projects/img_to_spec/large_files/ILSVRC/images_10_class/000_tench/00000.jpg\"\n",
    "DEVICE = \"cuda\"\n",
    "NUM_INFERENCE_STEPS = 100\n",
    "\n",
    "# Image preprocessing\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def patchify_image(image, patch_size=16):\n",
    "    C, H, W = image.shape\n",
    "    P = patch_size\n",
    "    x = image.reshape(C, H // P, P, W // P, P)\n",
    "    x = x.permute(1, 3, 0, 2, 4)\n",
    "    x = x.reshape((H // P) * (W // P), C * P * P)\n",
    "    return x\n",
    "\n",
    "def load_model(checkpoint_path, device=\"cuda\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    config = checkpoint['config']\n",
    "    model = SimpleDiT(**config).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"✓ Loaded model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference_fixed(\n",
    "    model,\n",
    "    scheduler,\n",
    "    mel_adapter,\n",
    "    image_patches,\n",
    "    num_inference_steps=50,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"Fixed DDIM inference.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    latents = torch.randn((1, 875, 32), device=device)\n",
    "    \n",
    "    # Create proper timestep schedule\n",
    "    step_size = max(1, scheduler.num_timesteps // num_inference_steps)\n",
    "    timesteps = list(range(scheduler.num_timesteps - 1, -1, -step_size))[:num_inference_steps]\n",
    "    if timesteps[-1] != 0:\n",
    "        timesteps.append(0)\n",
    "    \n",
    "    print(f\"Timesteps ({len(timesteps)}): {timesteps[:5]} ... {timesteps[-5:]}\")\n",
    "    \n",
    "    for i in range(len(timesteps) - 1):\n",
    "        t = timesteps[i]\n",
    "        prev_t = timesteps[i + 1]\n",
    "        \n",
    "        t_tensor = torch.tensor([t], device=device, dtype=torch.long)\n",
    "        \n",
    "        predicted_noise = model(latents, image_patches, t_tensor)\n",
    "        \n",
    "        # DDIM step\n",
    "        alpha_bar_t = scheduler.alphas_cumprod[t]\n",
    "        alpha_bar_prev = scheduler.alphas_cumprod[prev_t] if prev_t > 0 else torch.tensor(1.0, device=device)\n",
    "        \n",
    "        sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)\n",
    "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)\n",
    "        \n",
    "        # Predict x0\n",
    "        x0_pred = (latents - sqrt_one_minus_alpha_bar_t * predicted_noise) / sqrt_alpha_bar_t\n",
    "        x0_pred = torch.clamp(x0_pred, -10, 10)\n",
    "        \n",
    "        # Compute x_{prev_t}\n",
    "        sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)\n",
    "        sqrt_one_minus_alpha_bar_prev = torch.sqrt(1 - alpha_bar_prev)\n",
    "        \n",
    "        latents = sqrt_alpha_bar_prev * x0_pred + sqrt_one_minus_alpha_bar_prev * predicted_noise\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(f\"  Step {i:3d}: t={t:4d}→{prev_t:4d}, x0=[{x0_pred.min():.1f},{x0_pred.max():.1f}], latent=[{latents.min():.1f},{latents.max():.1f}]\")\n",
    "    \n",
    "    mel_spec = mel_adapter.unpack(latents, H=100, W=280)\n",
    "    return mel_spec\n",
    "\n",
    "\n",
    "# Load components\n",
    "model = load_model(MODEL_PATH)\n",
    "mel_adapter = MelAdapter(patch_freq=4, patch_time=8)\n",
    "noise_scheduler = NoiseScheduler(num_timesteps=1000, device=DEVICE)\n",
    "\n",
    "# Load image\n",
    "image = Image.open(INPUT_IMAGE).convert(\"RGB\")\n",
    "image = image_transform(image)\n",
    "image_patches = patchify_image(image).unsqueeze(0).to(DEVICE)\n",
    "print(f\"Image patches: {image_patches.shape}\")\n",
    "\n",
    "# Generate\n",
    "generated_mel = run_inference_fixed(\n",
    "    model=model,\n",
    "    scheduler=noise_scheduler,\n",
    "    mel_adapter=mel_adapter,\n",
    "    image_patches=image_patches,\n",
    "    num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Convert to audio\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CONVERTING TO AUDIO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "vocos = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(DEVICE)\n",
    "vocos.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    mel_for_vocos = generated_mel.squeeze(1)\n",
    "    waveform = vocos.decode(mel_for_vocos)\n",
    "\n",
    "output_path = \"/home/vladimir_albrekht/projects/img_to_spec/src/inference_output/generated_audio_fixed.wav\"\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "torchaudio.save(output_path, waveform.cpu(), sample_rate=24000)\n",
    "\n",
    "print(f\"✓ Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model, loss: 0.0132\n",
      "Real mel shape: torch.Size([1, 1, 100, 280])\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC TEST: Single-step denoising\n",
      "============================================================\n",
      "t= 100 | Noise MSE: 0.0213 | Recovery MSE: 0.0025\n",
      "t= 300 | Noise MSE: 0.0150 | Recovery MSE: 0.0230\n",
      "t= 500 | Noise MSE: 0.0106 | Recovery MSE: 0.1254\n",
      "t= 700 | Noise MSE: 0.0048 | Recovery MSE: 0.6870\n",
      "t= 900 | Noise MSE: 0.0014 | Recovery MSE: 5.1369\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC: Generate from scratch vs from noisy real\n",
      "============================================================\n",
      "✓ Saved original.wav\n",
      "✓ Saved recovered_t100.wav\n",
      "\n",
      "→ Compare original.wav and recovered_t100.wav\n",
      "  If they sound similar, model is learning!\n",
      "  If very different, need more training.\n"
     ]
    }
   ],
   "source": [
    "# diagnostic_test.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from debug_training_code import MelAdapter, NoiseScheduler, SimpleDiT, AudioProcessor\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from vocos import Vocos\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "MODEL_PATH = \"/home/vladimir_albrekht/projects/img_to_spec/src/output/test_1/dit_checkpoint.pt\"\n",
    "\n",
    "# Load model\n",
    "def load_model(checkpoint_path, device=\"cuda\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    config = checkpoint['config']\n",
    "    model = SimpleDiT(**config).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"✓ Loaded model, loss: {checkpoint['loss']:.4f}\")\n",
    "    return model\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "mel_adapter = MelAdapter(patch_freq=4, patch_time=8)\n",
    "noise_scheduler = NoiseScheduler(num_timesteps=1000, device=DEVICE)\n",
    "\n",
    "# Load REAL mel from training data\n",
    "audio_processor = AudioProcessor(target_sr=24000, target_duration=3.0, device=\"cpu\")\n",
    "real_mel = audio_processor.process_file(\n",
    "    \"/home/vladimir_albrekht/projects/img_to_spec/large_files/ILSVRC/audio_10_class/000_tench/description.wav\"\n",
    ")\n",
    "real_mel = real_mel.unsqueeze(0)  # [1, 1, 100, frames]\n",
    "\n",
    "# Pad/trim\n",
    "TARGET_FRAMES = 280\n",
    "if real_mel.shape[-1] < TARGET_FRAMES:\n",
    "    real_mel = F.pad(real_mel, (0, TARGET_FRAMES - real_mel.shape[-1]))\n",
    "else:\n",
    "    real_mel = real_mel[..., :TARGET_FRAMES]\n",
    "\n",
    "real_mel = real_mel.to(DEVICE)\n",
    "print(f\"Real mel shape: {real_mel.shape}\")\n",
    "\n",
    "# Load corresponding image\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def patchify_image(image, patch_size=16):\n",
    "    C, H, W = image.shape\n",
    "    P = patch_size\n",
    "    x = image.reshape(C, H // P, P, W // P, P)\n",
    "    x = x.permute(1, 3, 0, 2, 4)\n",
    "    x = x.reshape((H // P) * (W // P), C * P * P)\n",
    "    return x\n",
    "\n",
    "image = Image.open(\"/home/vladimir_albrekht/projects/img_to_spec/large_files/ILSVRC/images_10_class/000_tench/00000.jpg\").convert(\"RGB\")\n",
    "image = image_transform(image)\n",
    "image_patches = patchify_image(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIAGNOSTIC TEST: Single-step denoising\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test at different timesteps\n",
    "for t_val in [100, 300, 500, 700, 900]:\n",
    "    clean_patches = mel_adapter.pack(real_mel)\n",
    "    \n",
    "    t = torch.tensor([t_val], device=DEVICE)\n",
    "    noisy_patches, true_noise = noise_scheduler.add_noise(clean_patches, t)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_noise = model(noisy_patches, image_patches, t)\n",
    "    \n",
    "    # MSE between predicted and true noise\n",
    "    noise_mse = F.mse_loss(predicted_noise, true_noise)\n",
    "    \n",
    "    # Try to recover original\n",
    "    alpha_cumprod = noise_scheduler.alphas_cumprod[t_val]\n",
    "    sqrt_alpha = torch.sqrt(alpha_cumprod)\n",
    "    sqrt_one_minus_alpha = torch.sqrt(1 - alpha_cumprod)\n",
    "    \n",
    "    recovered = (noisy_patches - sqrt_one_minus_alpha * predicted_noise) / sqrt_alpha\n",
    "    recovery_mse = F.mse_loss(recovered, clean_patches)\n",
    "    \n",
    "    print(f\"t={t_val:4d} | Noise MSE: {noise_mse.item():.4f} | Recovery MSE: {recovery_mse.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIAGNOSTIC: Generate from scratch vs from noisy real\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Single step recovery from t=100 (light noise)\n",
    "t = torch.tensor([800], device=DEVICE)\n",
    "clean_patches = mel_adapter.pack(real_mel)\n",
    "noisy_patches, true_noise = noise_scheduler.add_noise(clean_patches, t)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_noise = model(noisy_patches, image_patches, t)\n",
    "\n",
    "alpha_cumprod = noise_scheduler.alphas_cumprod[800]\n",
    "recovered = (noisy_patches - torch.sqrt(1 - alpha_cumprod) * predicted_noise) / torch.sqrt(alpha_cumprod)\n",
    "recovered_mel = mel_adapter.unpack(recovered, H=100, W=280)\n",
    "\n",
    "# Save recovered audio\n",
    "vocos = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(DEVICE)\n",
    "vocos.eval()\n",
    "\n",
    "output_dir = Path(\"/home/vladimir_albrekht/projects/img_to_spec/src/inference_output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save original\n",
    "with torch.no_grad():\n",
    "    orig_audio = vocos.decode(real_mel.squeeze(1))\n",
    "torchaudio.save(str(output_dir / \"original.wav\"), orig_audio.cpu(), 24000)\n",
    "print(f\"✓ Saved original.wav\")\n",
    "\n",
    "# Save recovered from t=100\n",
    "with torch.no_grad():\n",
    "    rec_audio = vocos.decode(recovered_mel.squeeze(1))\n",
    "torchaudio.save(str(output_dir / \"recovered_t100.wav\"), rec_audio.cpu(), 24000)\n",
    "print(f\"✓ Saved recovered_t100.wav\")\n",
    "\n",
    "print(\"\\n→ Compare original.wav and recovered_t100.wav\")\n",
    "print(\"  If they sound similar, model is learning!\")\n",
    "print(\"  If very different, need more training.\")\n",
    "\n",
    "\n",
    "## Ожидаемые Результаты Diagnostic\n",
    "\n",
    "# **Если модель учится правильно:**\n",
    "# ```\n",
    "# t= 100 | Noise MSE: 0.05 | Recovery MSE: 0.02\n",
    "# t= 500 | Noise MSE: 0.08 | Recovery MSE: 0.10\n",
    "# t= 900 | Noise MSE: 0.12 | Recovery MSE: 0.50\n",
    "# ```\n",
    "\n",
    "# **Если модель НЕ учится:**\n",
    "# ```\n",
    "# t= 100 | Noise MSE: 0.50 | Recovery MSE: 0.80\n",
    "# t= 500 | Noise MSE: 0.60 | Recovery MSE: 1.20\n",
    "\n",
    "\n",
    "### with 5 epoch\n",
    "# ============================================================\n",
    "# DIAGNOSTIC TEST: Single-step denoising\n",
    "# ============================================================\n",
    "# t= 100 | Noise MSE: 0.3247 | Recovery MSE: 0.0380\n",
    "# t= 300 | Noise MSE: 0.1129 | Recovery MSE: 0.1737\n",
    "# t= 500 | Noise MSE: 0.0545 | Recovery MSE: 0.6460\n",
    "# t= 700 | Noise MSE: 0.0184 | Recovery MSE: 2.6605\n",
    "# t= 900 | Noise MSE: 0.0057 | Recovery MSE: 20.9211\n",
    "\n",
    "\n",
    "# with 50 epochs\n",
    "# ============================================================\n",
    "# DIAGNOSTIC TEST: Single-step denoising\n",
    "# ============================================================\n",
    "# t= 100 | Noise MSE: 0.0208 | Recovery MSE: 0.0024\n",
    "# t= 300 | Noise MSE: 0.0145 | Recovery MSE: 0.0223\n",
    "# t= 500 | Noise MSE: 0.0092 | Recovery MSE: 0.1093\n",
    "# t= 700 | Noise MSE: 0.0044 | Recovery MSE: 0.6354\n",
    "# t= 900 | Noise MSE: 0.0012 | Recovery MSE: 4.6006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading components on cuda...\n",
      "✓ Saved 0_original_clean.wav\n",
      "--------------------------------------------------\n",
      "✓ Saved noisy_input_t100.wav\n",
      "   -> Signal: 94.6% | Noise: 32.4%\n",
      "✓ Saved noisy_input_t300.wav\n",
      "   -> Signal: 62.8% | Noise: 77.8%\n",
      "✓ Saved noisy_input_t500.wav\n",
      "   -> Signal: 27.9% | Noise: 96.0%\n",
      "✓ Saved noisy_input_t800.wav\n",
      "   -> Signal: 3.9% | Noise: 99.9%\n",
      "--------------------------------------------------\n",
      "Done! Check the folder: noise_test_output\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from vocos import Vocos\n",
    "from pathlib import Path\n",
    "from debug_training_code import AudioProcessor, NoiseScheduler, MelAdapter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. SETUP\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = Path(\"./noise_test_output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path to the real audio file you used in the previous test\n",
    "AUDIO_PATH = \"/home/vladimir_albrekht/projects/img_to_spec/large_files/ILSVRC/audio_10_class/000_tench/description.wav\"\n",
    "\n",
    "print(f\"Loading components on {DEVICE}...\")\n",
    "\n",
    "# 2. INIT COMPONENTS\n",
    "audio_processor = AudioProcessor(target_sr=24000, target_duration=3.0, device=\"cpu\")\n",
    "vocos = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(DEVICE)\n",
    "noise_scheduler = NoiseScheduler(num_timesteps=1000, device=DEVICE)\n",
    "mel_adapter = MelAdapter(patch_freq=4, patch_time=8)\n",
    "\n",
    "# 3. PREPARE ORIGINAL AUDIO\n",
    "# Load and Process Mel\n",
    "original_mel = audio_processor.process_file(AUDIO_PATH)\n",
    "original_mel = original_mel.unsqueeze(0) # [1, 1, 100, T]\n",
    "\n",
    "# Pad/Trim to standard size (important for adapter)\n",
    "TARGET_FRAMES = 280\n",
    "if original_mel.shape[-1] < TARGET_FRAMES:\n",
    "    original_mel = F.pad(original_mel, (0, TARGET_FRAMES - original_mel.shape[-1]))\n",
    "else:\n",
    "    original_mel = original_mel[..., :TARGET_FRAMES]\n",
    "\n",
    "original_mel = original_mel.to(DEVICE)\n",
    "\n",
    "# Save the pure original for reference\n",
    "with torch.no_grad():\n",
    "    clean_audio = vocos.decode(original_mel.squeeze(1))\n",
    "torchaudio.save(OUTPUT_DIR / \"0_original_clean.wav\", clean_audio.cpu(), 24000)\n",
    "print(f\"✓ Saved 0_original_clean.wav\")\n",
    "\n",
    "# 4. GENERATE NOISY VERSIONS\n",
    "timesteps_to_test = [100, 300, 500, 800]\n",
    "\n",
    "print(\"-\" * 50)\n",
    "for t_val in timesteps_to_test:\n",
    "    # A. Pack Mel into Patches (so we can add noise exactly like training)\n",
    "    clean_patches = mel_adapter.pack(original_mel) # [1, 875, 32]\n",
    "    \n",
    "    # B. Add Noise\n",
    "    t = torch.tensor([t_val], device=DEVICE)\n",
    "    noisy_patches, _ = noise_scheduler.add_noise(clean_patches, t)\n",
    "    \n",
    "    # C. Unpack back to Mel Spectrogram\n",
    "    noisy_mel = mel_adapter.unpack(noisy_patches, H=100, W=280)\n",
    "    \n",
    "    # D. Decode to Audio using Vocos\n",
    "    with torch.no_grad():\n",
    "        noisy_audio = vocos.decode(noisy_mel.squeeze(1))\n",
    "    \n",
    "    # E. Save\n",
    "    filename = f\"noisy_input_t{t_val}.wav\"\n",
    "    save_path = OUTPUT_DIR / filename\n",
    "    torchaudio.save(str(save_path), noisy_audio.cpu(), 24000)\n",
    "    \n",
    "    print(f\"✓ Saved {filename}\")\n",
    "    \n",
    "    # Math check for signal strength\n",
    "    alpha_bar = noise_scheduler.alphas_cumprod[t_val].item()\n",
    "    signal_strength = torch.sqrt(torch.tensor(alpha_bar)).item()\n",
    "    noise_strength = torch.sqrt(torch.tensor(1 - alpha_bar)).item()\n",
    "    print(f\"   -> Signal: {signal_strength*100:.1f}% | Noise: {noise_strength*100:.1f}%\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Done! Check the folder: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
