# i2m

## uv

```bash
uv venv --python 3.13 --seed .venv
uv pip install huggingface_hub
uv pip install python-dotenv
python -m ipykernel install --user --name i2m
uv pip install \
        torch==2.9.0 \
        torchvision==0.24.0 \
        torchaudio==2.9.0 \
        transformers==4.57.3 \
        pandas==2.3.2 \
        python-dotenv==1.2.1 \
        matplotlib==3.10.8 \
        accelerate==1.12.0 \
        ipykernel \
        piper-tts \
        vocos

```


1. tar

`tar -czf images_10_class.tar.gz -C /home/vladimir_albrekht/projects/img_to_spec/large_files/ILSVRC images_10_class`

### TODO:
1. Let's add Flash-attention if that possible for 'self_attention' and 'cross-attention'
2. 


## README Content

---

### ðŸ“Š Dataset

```
DATASET STRUCTURE
=================

10 ImageNet Classes:
â”œâ”€â”€ tench, goldfish, great white shark, tiger shark, hammerhead
â”œâ”€â”€ electric ray, stingray, rooster, hen, ostrich
â””â”€â”€ ~1,300 images per class = 13,000 total images

Audio Generation:
â”œâ”€â”€ 20 TTS speakers (Piper TTS)
â”œâ”€â”€ Speed augmentation: 0.75x - 1.35x
â”œâ”€â”€ Pitch augmentation: -4 to +4 semitones
â””â”€â”€ 13,000 unique audio files (3 sec, 24kHz)

Train/Eval Split:
â”œâ”€â”€ Train: 11,700 pairs (90%)
â””â”€â”€ Eval:   1,300 pairs (10%)

Each pair:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image (512Ã—512)â”‚  â†’   â”‚ Mel Spectrogram â”‚
â”‚    (fish.jpg)   â”‚      â”‚   (100Ã—280)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ðŸ—ï¸ Architecture

```
SimpleDiT: Image-to-Audio Diffusion Transformer
===============================================

INPUT PROCESSING:
                                                    
 Image (512Ã—512Ã—3)          Mel Spectrogram (100Ã—280)
        â”‚                            â”‚
        â–¼                            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Patchify â”‚                 â”‚  Patchify â”‚
   â”‚ 16Ã—16   â”‚                 â”‚  4Ã—8      â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                            â”‚
        â–¼                            â–¼
  [1024, 768]                   [875, 32]
  1024 patches                  875 patches
        â”‚                            â”‚
        â–¼                            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Linear   â”‚                 â”‚  Linear   â”‚
   â”‚Projection                 â”‚ Projectionâ”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                            â”‚
        â–¼                            â–¼
  [1024, 768]                   [875, 768]
   + pos_embed                   + pos_embed
        â”‚                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚    â”‚
                   â–¼    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   DiT Block Ã—12 â”‚
            â”‚                 â”‚
            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚ â”‚ Self-Attn   â”‚â—„â”€â”€ Mel attends to Mel
            â”‚ â”‚ (Melâ†’Mel)   â”‚ â”‚
            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â”‚        â”‚        â”‚
            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚ â”‚ Cross-Attn  â”‚â—„â”€â”€ Mel attends to Image
            â”‚ â”‚ (Melâ†’Image) â”‚ â”‚
            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â”‚        â”‚        â”‚
            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ â”‚    MLP      â”‚ â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”‚ Timestep  â”‚
            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚ Embedding â”‚
            â”‚                 â”‚        â”‚  (t=500)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Linear    â”‚
              â”‚  Projection â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              Predicted Noise
                [875, 32]
                     â”‚
                     â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Unpatchifyâ”‚
               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              Mel (100Ã—280)


MODEL CONFIG (150M params):
===========================
â€¢ Hidden size: 768
â€¢ Layers: 12
â€¢ Attention heads: 12
â€¢ Mel patches: 875 (25Ã—35)
â€¢ Image patches: 1024 (32Ã—32)


TRAINING:
=========
â€¢ Noise schedule: Linear Î² (0.0001 â†’ 0.02)
â€¢ Timesteps: 1000
â€¢ Condition dropout: 10% (for CFG)
â€¢ Optimizer: AdamW
â€¢ Scheduler: Cosine Annealing


INFERENCE:
==========
â€¢ Sampler: DDIM (50-100 steps)
â€¢ CFG scale: 1.0 - 7.5
â€¢ Audio reconstruction: Vocos vocoder
```

---

### ðŸŽ¯ Project Summary

```
GOAL
====
Generate audio descriptions from images using diffusion models.

Image of a fish  â†’  "A goldfish, bright orange fish" (spoken audio)


CURRENT STATUS
==============
âœ… Single-step denoising works well (MSE ~0.007)
âŒ Full generation produces wrong class (mode collapse)
âŒ Model ignores image condition


ROOT CAUSE
==========
Original dataset: 13,000 images â†’ only 10 unique audio files
Model learned: "ignore image, generate average mel"


SOLUTION IN PROGRESS
====================
New dataset with diversity:
â€¢ 20 speakers Ã— 10 classes = 200 base audios
â€¢ Speed/pitch augmentation = 13,000 unique mels
â€¢ Condition dropout (10%) for CFG training
```

---

### ðŸ—ºï¸ Roadmap

```
TODO LIST
=========

[âœ…] DONE:
    â€¢ Basic DiT architecture
    â€¢ Training pipeline with DDP
    â€¢ Vocos mel extraction/reconstruction
    â€¢ Single-step denoising validation
    â€¢ Train/eval split

[ðŸ”„] IN PROGRESS:
    â€¢ Multi-speaker audio generation (20 speakers)
    â€¢ Speed/pitch augmentations
    â€¢ Condition dropout for CFG

[ðŸ“‹] NEXT:
    â–¡ Train on augmented dataset (500 epochs)
    â–¡ Evaluate with CFG (scale 1.0, 3.0, 5.0, 7.5)
    â–¡ Confusion matrix evaluation
    â–¡ Compare: random baseline (10%) vs model accuracy

[ðŸ”®] FUTURE IMPROVEMENTS:
    â–¡ EMA weights tracking
    â–¡ Cosine noise schedule
    â–¡ Larger model (300M+ params)
    â–¡ More classes (100+)
    â–¡ Real image-audio pairs (video datasets)
    â–¡ CLIP image encoder instead of raw patches


EXPERIMENTS TO TRY
==================
1. CFG scale sweep: Find optimal guidance strength
2. More speakers: 50+ voices for more diversity  
3. Text variations: Multiple descriptions per class
4. Noise schedule: Try cosine instead of linear
5. Architecture: Add CLIP encoder for better image features
```

---

### ðŸ“ˆ Metrics

```
EVALUATION METRICS
==================

1. Denoising Quality (should be low):
   â€¢ MSE at t=300: ~0.007 âœ…
   â€¢ MSE at t=500: ~0.03  âœ…
   â€¢ MSE at t=800: ~0.25  âœ…

2. Generation Quality (currently broken):
   â€¢ MSE vs ground truth: ~45-50 âŒ
   â€¢ Classification accuracy: 10% (random) âŒ

3. Target After Fix:
   â€¢ Classification accuracy: >50%
   â€¢ Distinct mel per class
```